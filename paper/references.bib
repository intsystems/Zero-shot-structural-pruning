@Article{Sun2023,
  author        = {Sun, Mingjie and Liu, Zhuang and Bair, Anna and Kolter, J. Zico},
  title         = {A Simple and Effective Pruning Approach for Large Language Models},
  year          = {2023},
  month         = jun,
  abstract      = {As their size increases, Large Languages Models (LLMs) are natural candidates for network pruning methods: approaches that drop a subset of network weights while striving to preserve performance. Existing methods, however, require either retraining, which is rarely affordable for billion-scale LLMs, or solving a weight reconstruction problem reliant on second-order information, which may also be computationally expensive. In this paper, we introduce a novel, straightforward yet effective pruning method, termed Wanda (Pruning by Weights and activations), designed to induce sparsity in pretrained LLMs. Motivated by the recent observation of emergent large magnitude features in LLMs, our approach prunes weights with the smallest magnitudes multiplied by the corresponding input activations, on a per-output basis. Notably, Wanda requires no retraining or weight update, and the pruned LLM can be used as is. We conduct a thorough evaluation of our method Wanda on LLaMA and LLaMA-2 across various language benchmarks. Wanda significantly outperforms the established baseline of magnitude pruning and performs competitively against recent method involving intensive weight update. Code is available at https://github.com/locuslab/wanda.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.2306.11695},
  eprint        = {2306.11695},
  file          = {:http\://arxiv.org/pdf/2306.11695v3:PDF},
  keywords      = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences},
  primaryclass  = {cs.CL},
  publisher     = {arXiv},
}

@InProceedings{inproceedings,
  author  = {Lecun, Yann and Denker, John and Solla, Sara},
  title   = {Optimal Brain Damage},
  year    = {1989},
  month   = {01},
  pages   = {598-605},
  volume  = {2},
  journal = {Advances in Neural Information Processing Systems},
}

@Article{Han2015,
  author        = {Han, Song and Pool, Jeff and Tran, John and Dally, William J.},
  title         = {Learning both Weights and Connections for Efficient Neural Networks},
  year          = {2015},
  month         = jun,
  abstract      = {Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems. Also, conventional networks fix the architecture before training starts; as a result, training cannot improve the architecture. To address these limitations, we describe a method to reduce the storage and computation required by neural networks by an order of magnitude without affecting their accuracy by learning only the important connections. Our method prunes redundant connections using a three-step method. First, we train the network to learn which connections are important. Next, we prune the unimportant connections. Finally, we retrain the network to fine tune the weights of the remaining connections. On the ImageNet dataset, our method reduced the number of parameters of AlexNet by a factor of 9x, from 61 million to 6.7 million, without incurring accuracy loss. Similar experiments with VGG-16 found that the number of parameters can be reduced by 13x, from 138 million to 10.3 million, again with no loss of accuracy.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.1506.02626},
  eprint        = {1506.02626},
  file          = {:Han2015 - Learning Both Weights and Connections for Efficient Neural Networks.pdf:PDF:http\://arxiv.org/pdf/1506.02626v3},
  keywords      = {Neural and Evolutionary Computing (cs.NE), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Computer and information sciences},
  primaryclass  = {cs.NE},
  publisher     = {arXiv},
}

@Article{Chen2021,
  author        = {Chen, Tianyi and Ji, Bo and Ding, Tianyu and Fang, Biyi and Wang, Guanyi and Zhu, Zhihui and Liang, Luming and Shi, Yixin and Yi, Sheng and Tu, Xiao},
  title         = {Only Train Once: A One-Shot Neural Network Training And Pruning Framework},
  year          = {2021},
  month         = jul,
  abstract      = {Structured pruning is a commonly used technique in deploying deep neural networks (DNNs) onto resource-constrained devices. However, the existing pruning methods are usually heuristic, task-specified, and require an extra fine-tuning procedure. To overcome these limitations, we propose a framework that compresses DNNs into slimmer architectures with competitive performances and significant FLOPs reductions by Only-Train-Once (OTO). OTO contains two keys: (i) we partition the parameters of DNNs into zero-invariant groups, enabling us to prune zero groups without affecting the output; and (ii) to promote zero groups, we then formulate a structured-sparsity optimization problem and propose a novel optimization algorithm, Half-Space Stochastic Projected Gradient (HSPG), to solve it, which outperforms the standard proximal methods on group sparsity exploration and maintains comparable convergence. To demonstrate the effectiveness of OTO, we train and compress full models simultaneously from scratch without fine-tuning for inference speedup and parameter reduction, and achieve state-of-the-art results on VGG16 for CIFAR10, ResNet50 for CIFAR10 and Bert for SQuAD and competitive result on ResNet50 for ImageNet. The source code is available at https://github.com/tianyic/only_train_once.},
  archiveprefix = {arXiv},
  copyright     = {Creative Commons Attribution 4.0 International},
  doi           = {10.48550/ARXIV.2107.07467},
  eprint        = {2107.07467},
  file          = {:Chen2021 - Only Train Once_ a One Shot Neural Network Training and Pruning Framework.pdf:PDF:http\://arxiv.org/pdf/2107.07467v2},
  keywords      = {Machine Learning (cs.LG), FOS: Computer and information sciences},
  primaryclass  = {cs.LG},
  publisher     = {arXiv},
}

@InProceedings{pmlr-v221-laenen23a,
  author    = {Laenen, Steinar},
  booktitle = {Proceedings of 2nd Annual Workshop on Topology, Algebra, and Geometry in Machine Learning (TAG-ML)},
  title     = {One-Shot Neural Network Pruning via Spectral Graph Sparsification},
  year      = {2023},
  editor    = {Doster, Timothy and Emerson, Tegan and Kvinge, Henry and Miolane, Nina and Papillon, Mathilde and Rieck, Bastian and Sanborn, Sophia},
  month     = {28 Jul},
  pages     = {60--71},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {221},
  abstract  = {Neural network pruning has gained significant attention for its potential to reduce computational resources required for training and inference. A large body of research has shown that networks can be pruned both after training and at initialisation, while maintaining competitive accuracy compared to dense networks. However, current methods rely on iteratively pruning or repairing the network to avoid over-pruning and layer collapse. Recent work has found that by treating neural networks as a sequence of bipartite graphs, pruning can be studied through the lens of spectral graph theory. Therefore, in this work, we propose a novel pruning approach using spectral sparsification, which aims to preserve meaningful properties of a dense graph with a sparse subgraph, by preserving the spectrum of the dense graph’s adjacency matrix. We empirically validate and investigate our method, and show that one-shot pruning using spectral sparsification preserves performance at higher levels of sparsity compared to its one-shot counterparts. Additionally, we theoretically analyse our method with respect to local and global connectivity.},
  pdf       = {https://proceedings.mlr.press/v221/laenen23a/laenen23a.pdf},
  url       = {https://proceedings.mlr.press/v221/laenen23a.html},
}

@Article{Tanaka2020,
  author        = {Tanaka, Hidenori and Kunin, Daniel and Yamins, Daniel L. K. and Ganguli, Surya},
  journal       = {Advances in Neural Information Processing Systems 2020},
  title         = {Pruning neural networks without any data by iteratively conserving synaptic flow},
  year          = {2020},
  month         = jun,
  abstract      = {Pruning the parameters of deep neural networks has generated intense interest due to potential savings in time, memory and energy both during training and at test time. Recent works have identified, through an expensive sequence of training and pruning cycles, the existence of winning lottery tickets or sparse trainable subnetworks at initialization. This raises a foundational question: can we identify highly sparse trainable subnetworks at initialization, without ever training, or indeed without ever looking at the data? We provide an affirmative answer to this question through theory driven algorithm design. We first mathematically formulate and experimentally verify a conservation law that explains why existing gradient-based pruning algorithms at initialization suffer from layer-collapse, the premature pruning of an entire layer rendering a network untrainable. This theory also elucidates how layer-collapse can be entirely avoided, motivating a novel pruning algorithm Iterative Synaptic Flow Pruning (SynFlow). This algorithm can be interpreted as preserving the total flow of synaptic strengths through the network at initialization subject to a sparsity constraint. Notably, this algorithm makes no reference to the training data and consistently competes with or outperforms existing state-of-the-art pruning algorithms at initialization over a range of models (VGG and ResNet), datasets (CIFAR-10/100 and Tiny ImageNet), and sparsity constraints (up to 99.99 percent). Thus our data-agnostic pruning algorithm challenges the existing paradigm that, at initialization, data must be used to quantify which synapses are important.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.2006.05467},
  eprint        = {2006.05467},
  file          = {:Tanaka2020 - Pruning Neural Networks without Any Data by Iteratively Conserving Synaptic Flow.pdf:PDF:http\://arxiv.org/pdf/2006.05467v3},
  keywords      = {Machine Learning (cs.LG), Disordered Systems and Neural Networks (cond-mat.dis-nn), Computer Vision and Pattern Recognition (cs.CV), Neurons and Cognition (q-bio.NC), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Physical sciences, FOS: Biological sciences},
  primaryclass  = {cs.LG},
  publisher     = {arXiv},
}

@Article{Fang2023,
  author        = {Fang, Gongfan and Ma, Xinyin and Song, Mingli and Mi, Michael Bi and Wang, Xinchao},
  title         = {DepGraph: Towards Any Structural Pruning},
  year          = {2023},
  month         = jan,
  abstract      = {Structural pruning enables model acceleration by removing structurally-grouped parameters from neural networks. However, the parameter-grouping patterns vary widely across different models, making architecture-specific pruners, which rely on manually-designed grouping schemes, non-generalizable to new architectures. In this work, we study a highly-challenging yet barely-explored task, any structural pruning, to tackle general structural pruning of arbitrary architecture like CNNs, RNNs, GNNs and Transformers. The most prominent obstacle towards this goal lies in the structural coupling, which not only forces different layers to be pruned simultaneously, but also expects all removed parameters to be consistently unimportant, thereby avoiding structural issues and significant performance degradation after pruning. To address this problem, we propose a general and {fully automatic} method, \emph{Dependency Graph} (DepGraph), to explicitly model the dependency between layers and comprehensively group coupled parameters for pruning. In this work, we extensively evaluate our method on several architectures and tasks, including ResNe(X)t, DenseNet, MobileNet and Vision transformer for images, GAT for graph, DGCNN for 3D point cloud, alongside LSTM for language, and demonstrate that, even with a simple norm-based criterion, the proposed method consistently yields gratifying performances.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.2301.12900},
  eprint        = {2301.12900},
  file          = {:Fang2023 - DepGraph_ Towards Any Structural Pruning.pdf:PDF:http\://arxiv.org/pdf/2301.12900v2},
  keywords      = {Artificial Intelligence (cs.AI), Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences},
  primaryclass  = {cs.AI},
  publisher     = {arXiv},
}

@Article{Sevilla2022,
  author        = {Sevilla, Jaime and Heim, Lennart and Ho, Anson and Besiroglu, Tamay and Hobbhahn, Marius and Villalobos, Pablo},
  journal       = {2022 International Joint Conference on Neural Networks (IJCNN), Padua, Italy, 2022, pp. 1-8},
  title         = {Compute Trends Across Three Eras of Machine Learning},
  year          = {2022},
  month         = jul,
  pages         = {1--8},
  abstract      = {Compute, data, and algorithmic advances are the three fundamental factors that guide the progress of modern Machine Learning (ML). In this paper we study trends in the most readily quantified factor - compute. We show that before 2010 training compute grew in line with Moore's law, doubling roughly every 20 months. Since the advent of Deep Learning in the early 2010s, the scaling of training compute has accelerated, doubling approximately every 6 months. In late 2015, a new trend emerged as firms developed large-scale ML models with 10 to 100-fold larger requirements in training compute. Based on these observations we split the history of compute in ML into three eras: the Pre Deep Learning Era, the Deep Learning Era and the Large-Scale Era. Overall, our work highlights the fast-growing compute requirements for training advanced ML systems.},
  archiveprefix = {arXiv},
  booktitle     = {2022 International Joint Conference on Neural Networks (IJCNN)},
  copyright     = {Creative Commons Attribution 4.0 International},
  date          = {2022-02-11},
  doi           = {10.1109/ijcnn55064.2022.9891914},
  eprint        = {2202.05924},
  file          = {:Sevilla2022 - Compute Trends across Three Eras of Machine Learning.pdf:PDF:http\://arxiv.org/pdf/2202.05924v2},
  keywords      = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Computers and Society (cs.CY), FOS: Computer and information sciences},
  primaryclass  = {cs.LG},
  publisher     = {IEEE},
}

@Article{Thompson2020,
  author        = {Thompson, Neil C. and Greenewald, Kristjan and Lee, Keeheon and Manso, Gabriel F.},
  title         = {The Computational Limits of Deep Learning},
  year          = {2020},
  month         = jul,
  abstract      = {Deep learning's recent history has been one of achievement: from triumphing over humans in the game of Go to world-leading performance in image classification, voice recognition, translation, and other tasks. But this progress has come with a voracious appetite for computing power. This article catalogs the extent of this dependency, showing that progress across a wide variety of applications is strongly reliant on increases in computing power. Extrapolating forward this reliance reveals that progress along current lines is rapidly becoming economically, technically, and environmentally unsustainable. Thus, continued progress in these applications will require dramatically more computationally-efficient methods, which will either have to come from changes to deep learning or from moving to other machine learning methods.},
  archiveprefix = {arXiv},
  copyright     = {Creative Commons Attribution 4.0 International},
  doi           = {10.48550/ARXIV.2007.05558},
  eprint        = {2007.05558},
  file          = {:Thompson2020 - The Computational Limits of Deep Learning.pdf:PDF:http\://arxiv.org/pdf/2007.05558v2},
  keywords      = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences},
  primaryclass  = {cs.LG},
  publisher     = {arXiv},
}

@InProceedings{BanjanovicMehmedovic2023,
  author     = {Banjanović-Mehmedović, Lejla and Husaković, Anel},
  booktitle  = {BASIC TECHNOLOGIES AND MODELS FOR IMPLEMENTATION OF INDUSTRY 4.0},
  title      = {Edge AI: Reshaping the Future of Edge Computing with Artificial Intelligence},
  year       = {2023},
  month      = oct,
  pages      = {133--160},
  publisher  = {Academy of Sciences and Arts of Bosnia and Herzegovina},
  series     = {Basic 4.0},
  collection = {Basic 4.0},
  doi        = {10.5644/pi2023.209.07},
}

@Article{Blalock2020,
  author        = {Blalock, Davis and Ortiz, Jose Javier Gonzalez and Frankle, Jonathan and Guttag, John},
  title         = {What is the State of Neural Network Pruning?},
  year          = {2020},
  month         = mar,
  abstract      = {Neural network pruning---the task of reducing the size of a network by removing parameters---has been the subject of a great deal of work in recent years. We provide a meta-analysis of the literature, including an overview of approaches to pruning and consistent findings in the literature. After aggregating results across 81 papers and pruning hundreds of models in controlled conditions, our clearest finding is that the community suffers from a lack of standardized benchmarks and metrics. This deficiency is substantial enough that it is hard to compare pruning techniques to one another or determine how much progress the field has made over the past three decades. To address this situation, we identify issues with current practices, suggest concrete remedies, and introduce ShrinkBench, an open-source framework to facilitate standardized evaluations of pruning methods. We use ShrinkBench to compare various pruning techniques and show that its comprehensive evaluation can prevent common pitfalls when comparing pruning methods.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.2003.03033},
  eprint        = {2003.03033},
  file          = {:http\://arxiv.org/pdf/2003.03033v1:PDF},
  keywords      = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences},
  primaryclass  = {cs.LG},
  publisher     = {arXiv},
}

@Article{Molchanov2019,
  author        = {Molchanov, Pavlo and Mallya, Arun and Tyree, Stephen and Frosio, Iuri and Kautz, Jan},
  title         = {Importance Estimation for Neural Network Pruning},
  year          = {2019},
  month         = jun,
  abstract      = {Structural pruning of neural network parameters reduces computation, energy, and memory transfer costs during inference. We propose a novel method that estimates the contribution of a neuron (filter) to the final loss and iteratively removes those with smaller scores. We describe two variations of our method using the first and second-order Taylor expansions to approximate a filter's contribution. Both methods scale consistently across any network layer without requiring per-layer sensitivity analysis and can be applied to any kind of layer, including skip connections. For modern networks trained on ImageNet, we measured experimentally a high (>93%) correlation between the contribution computed by our methods and a reliable estimate of the true importance. Pruning with the proposed methods leads to an improvement over state-of-the-art in terms of accuracy, FLOPs, and parameter reduction. On ResNet-101, we achieve a 40% FLOPS reduction by removing 30% of the parameters, with a loss of 0.02% in the top-1 accuracy on ImageNet. Code is available at https://github.com/NVlabs/Taylor_pruning.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.1906.10771},
  eprint        = {1906.10771},
  file          = {:http\://arxiv.org/pdf/1906.10771v1:PDF},
  keywords      = {Machine Learning (cs.LG), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (stat.ML), FOS: Computer and information sciences},
  primaryclass  = {cs.LG},
  publisher     = {arXiv},
}

@InProceedings{Hassibi1992,
  author    = {Hassibi, Babak and Stork, David},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Second order derivatives for network pruning: Optimal Brain Surgeon},
  year      = {1992},
  editor    = {S. Hanson and J. Cowan and C. Giles},
  publisher = {Morgan-Kaufmann},
  volume    = {5},
  url       = {https://proceedings.neurips.cc/paper_files/paper/1992/file/303ed4c69846ab36c2904d3ba8573050-Paper.pdf},
}

@Misc{zeng2019mlprune,
  author = {Wenyuan Zeng and Raquel Urtasun},
  title  = {{MLP}rune: Multi-Layer Pruning for Automated Neural Network Compression},
  year   = {2019},
  url    = {https://openreview.net/forum?id=r1g5b2RcKm},
}

@Article{Lubana2020,
  author        = {Lubana, Ekdeep Singh and Dick, Robert P.},
  title         = {A Gradient Flow Framework For Analyzing Network Pruning},
  year          = {2020},
  month         = sep,
  abstract      = {Recent network pruning methods focus on pruning models early-on in training. To estimate the impact of removing a parameter, these methods use importance measures that were originally designed to prune trained models. Despite lacking justification for their use early-on in training, such measures result in surprisingly low accuracy loss. To better explain this behavior, we develop a general framework that uses gradient flow to unify state-of-the-art importance measures through the norm of model parameters. We use this framework to determine the relationship between pruning measures and evolution of model parameters, establishing several results related to pruning models early-on in training: (i) magnitude-based pruning removes parameters that contribute least to reduction in loss, resulting in models that converge faster than magnitude-agnostic methods; (ii) loss-preservation based pruning preserves first-order model evolution dynamics and is therefore appropriate for pruning minimally trained models; and (iii) gradient-norm based pruning affects second-order model evolution dynamics, such that increasing gradient norm via pruning can produce poorly performing models. We validate our claims on several VGG-13, MobileNet-V1, and ResNet-56 models trained on CIFAR-10/CIFAR-100. Code available at https://github.com/EkdeepSLubana/flowandprune.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.2009.11839},
  eprint        = {2009.11839},
  file          = {:http\://arxiv.org/pdf/2009.11839v4:PDF},
  keywords      = {Machine Learning (cs.LG), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (stat.ML), FOS: Computer and information sciences},
  primaryclass  = {cs.LG},
  publisher     = {arXiv},
}

@Article{He2018,
  author        = {He, Yang and Kang, Guoliang and Dong, Xuanyi and Fu, Yanwei and Yang, Yi},
  title         = {Soft Filter Pruning for Accelerating Deep Convolutional Neural Networks},
  year          = {2018},
  month         = aug,
  abstract      = {This paper proposed a Soft Filter Pruning (SFP) method to accelerate the inference procedure of deep Convolutional Neural Networks (CNNs). Specifically, the proposed SFP enables the pruned filters to be updated when training the model after pruning. SFP has two advantages over previous works: (1) Larger model capacity. Updating previously pruned filters provides our approach with larger optimization space than fixing the filters to zero. Therefore, the network trained by our method has a larger model capacity to learn from the training data. (2) Less dependence on the pre-trained model. Large capacity enables SFP to train from scratch and prune the model simultaneously. In contrast, previous filter pruning methods should be conducted on the basis of the pre-trained model to guarantee their performance. Empirically, SFP from scratch outperforms the previous filter pruning methods. Moreover, our approach has been demonstrated effective for many advanced CNN architectures. Notably, on ILSCRC-2012, SFP reduces more than 42% FLOPs on ResNet-101 with even 0.2% top-5 accuracy improvement, which has advanced the state-of-the-art. Code is publicly available on GitHub: https://github.com/he-y/soft-filter-pruning},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.1808.06866},
  eprint        = {1808.06866},
  file          = {:http\://arxiv.org/pdf/1808.06866v1:PDF},
  keywords      = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences},
  primaryclass  = {cs.CV},
  publisher     = {arXiv},
}

@Article{Molchanov2016,
  author        = {Molchanov, Pavlo and Tyree, Stephen and Karras, Tero and Aila, Timo and Kautz, Jan},
  title         = {Pruning Convolutional Neural Networks for Resource Efficient Inference},
  year          = {2016},
  month         = nov,
  abstract      = {We propose a new formulation for pruning convolutional kernels in neural networks to enable efficient inference. We interleave greedy criteria-based pruning with fine-tuning by backpropagation - a computationally efficient procedure that maintains good generalization in the pruned network. We propose a new criterion based on Taylor expansion that approximates the change in the cost function induced by pruning network parameters. We focus on transfer learning, where large pretrained networks are adapted to specialized tasks. The proposed criterion demonstrates superior performance compared to other criteria, e.g. the norm of kernel weights or feature map activation, for pruning large CNNs after adaptation to fine-grained classification tasks (Birds-200 and Flowers-102) relaying only on the first order gradient information. We also show that pruning can lead to more than 10x theoretical (5x practical) reduction in adapted 3D-convolutional filters with a small drop in accuracy in a recurrent gesture classifier. Finally, we show results for the large-scale ImageNet dataset to emphasize the flexibility of our approach.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.1611.06440},
  eprint        = {1611.06440},
  file          = {:http\://arxiv.org/pdf/1611.06440v2:PDF},
  keywords      = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences},
  primaryclass  = {cs.LG},
  publisher     = {arXiv},
}

@Article{Liu2021,
  author        = {Liu, Liyang and Zhang, Shilong and Kuang, Zhanghui and Zhou, Aojun and Xue, Jing-Hao and Wang, Xinjiang and Chen, Yimin and Yang, Wenming and Liao, Qingmin and Zhang, Wayne},
  title         = {Group Fisher Pruning for Practical Network Compression},
  year          = {2021},
  month         = aug,
  abstract      = {Network compression has been widely studied since it is able to reduce the memory and computation cost during inference. However, previous methods seldom deal with complicated structures like residual connections, group/depth-wise convolution and feature pyramid network, where channels of multiple layers are coupled and need to be pruned simultaneously. In this paper, we present a general channel pruning approach that can be applied to various complicated structures. Particularly, we propose a layer grouping algorithm to find coupled channels automatically. Then we derive a unified metric based on Fisher information to evaluate the importance of a single channel and coupled channels. Moreover, we find that inference speedup on GPUs is more correlated with the reduction of memory rather than FLOPs, and thus we employ the memory reduction of each channel to normalize the importance. Our method can be used to prune any structures including those with coupled channels. We conduct extensive experiments on various backbones, including the classic ResNet and ResNeXt, mobile-friendly MobileNetV2, and the NAS-based RegNet, both on image classification and object detection which is under-explored. Experimental results validate that our method can effectively prune sophisticated networks, boosting inference speed without sacrificing accuracy.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.2108.00708},
  eprint        = {2108.00708},
  file          = {:http\://arxiv.org/pdf/2108.00708v1:PDF},
  keywords      = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Computer and information sciences},
  primaryclass  = {cs.CV},
  publisher     = {arXiv},
}

@Article{Laurent2020,
  author        = {Laurent, César and Ballas, Camille and George, Thomas and Ballas, Nicolas and Vincent, Pascal},
  title         = {Revisiting Loss Modelling for Unstructured Pruning},
  year          = {2020},
  month         = jun,
  abstract      = {By removing parameters from deep neural networks, unstructured pruning methods aim at cutting down memory footprint and computational cost, while maintaining prediction accuracy. In order to tackle this otherwise intractable problem, many of these methods model the loss landscape using first or second order Taylor expansions to identify which parameters can be discarded. We revisit loss modelling for unstructured pruning: we show the importance of ensuring locality of the pruning steps. We systematically compare first and second order Taylor expansions and empirically show that both can reach similar levels of performance. Finally, we show that better preserving the original network function does not necessarily transfer to better performing networks after fine-tuning, suggesting that only considering the impact of pruning on the loss might not be a sufficient objective to design good pruning criteria.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.2006.12279},
  eprint        = {2006.12279},
  file          = {:http\://arxiv.org/pdf/2006.12279v1:PDF},
  keywords      = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences},
  primaryclass  = {cs.LG},
  publisher     = {arXiv},
}

@Article{Wang2019,
  author        = {Wang, Wei-Ting and Li, Han-Lin and Lin, Wei-Shiang and Chiang, Cheng-Ming and Tsai, Yi-Min},
  title         = {Architecture-aware Network Pruning for Vision Quality Applications},
  year          = {2019},
  month         = aug,
  abstract      = {Convolutional neural network (CNN) delivers impressive achievements in computer vision and machine learning field. However, CNN incurs high computational complexity, especially for vision quality applications because of large image resolution. In this paper, we propose an iterative architecture-aware pruning algorithm with adaptive magnitude threshold while cooperating with quality-metric measurement simultaneously. We show the performance improvement applied on vision quality applications and provide comprehensive analysis with flexible pruning configuration. With the proposed method, the Multiply-Accumulate (MAC) of state-of-the-art low-light imaging (SID) and super-resolution (EDSR) are reduced by 58% and 37% without quality drop, respectively. The memory bandwidth (BW) requirements of convolutional layer can be also reduced by 20% to 40%.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.1908.02125},
  eprint        = {1908.02125},
  file          = {:http\://arxiv.org/pdf/1908.02125v1:PDF},
  keywords      = {Image and Video Processing (eess.IV), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Electrical engineering, electronic engineering, information engineering, FOS: Computer and information sciences},
  primaryclass  = {eess.IV},
  publisher     = {arXiv},
}

@Comment{jabref-meta: databaseType:bibtex;}
