{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96b117f1",
   "metadata": {},
   "source": [
    "# Advanced-эксперименты с библиотекой DepGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679af224",
   "metadata": {},
   "source": [
    "[Ссылка](https://github.com/VainF/Torch-Pruning/) на репозиторий библиотеки."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce31d293",
   "metadata": {},
   "source": [
    "### Импорт модулей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d2f0760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.2/70.2 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.2/179.2 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m75.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m56.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m74.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "libcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\n",
      "pylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\n",
      "pylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install torch-pruning torcheval --upgrade -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f43a0859",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.fx import symbolic_trace\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torcheval.metrics import BinaryAUROC\n",
    "\n",
    "from torchvision.models import resnet50\n",
    "import torch_pruning as tp\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr, kendalltau\n",
    "\n",
    "import abc\n",
    "from typing import Callable, List, Tuple, Dict\n",
    "from functools import reduce, partial\n",
    "import re\n",
    "import copy\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import networkx\n",
    "\n",
    "sns.set_style('darkgrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e3b0c8",
   "metadata": {},
   "source": [
    "### Реализации модулей и функций"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85de47d",
   "metadata": {},
   "source": [
    "Данные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7ee7d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders(classes: List[int], batch_size: int = 16, img_size: int = 33, need_val: bool = False, cifar100: bool = False, train_limit = None):\n",
    "    classes_to_ids = {cls : i for i, cls in enumerate(classes)}\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.Resize(img_size),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.Resize(img_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "    if cifar100:\n",
    "        trainset = torchvision.datasets.CIFAR100(root='./data', train=True,\n",
    "                                            download=True, transform=transform_train)\n",
    "    else:\n",
    "        trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                            download=True, transform=transform_train)\n",
    "        \n",
    "    trainset = [(x, classes_to_ids[y]) for x, y in trainset if y in classes]\n",
    "    if need_val:\n",
    "        _trainset = trainset[:len(trainset)//2]\n",
    "        valset  = trainset[len(trainset)//2:]\n",
    "        trainset = _trainset\n",
    "    if train_limit:\n",
    "        trainset = trainset[:train_limit]\n",
    "        \n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                              shuffle=True)\n",
    "    if need_val:\n",
    "        valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size,\n",
    "                                              shuffle=True)\n",
    "    if cifar100:\n",
    "        testset = torchvision.datasets.CIFAR100(root='./data', train=False,\n",
    "                                           download=True, transform=transform_test)\n",
    "    else:\n",
    "        testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                           download=True, transform=transform_test)\n",
    "    \n",
    "    testset = [(x, classes_to_ids[y]) for x, y in testset if y in classes]\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                             shuffle=False)\n",
    "    if need_val:\n",
    "        return trainloader, valloader, testloader\n",
    "    return trainloader, testloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a219abbe",
   "metadata": {},
   "source": [
    "Циклы обучения и теста."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a50c6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model, traindata, testdata, epoch_num=1, lr=1e-3, device='cuda'):\n",
    "    history = []\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    batch_seen = 0\n",
    "    \n",
    "    for epoch_num in range(epoch_num):\n",
    "        losses = []\n",
    "        tq = tqdm(traindata, leave=False)\n",
    "        \n",
    "        for x, y in tq:\n",
    "            optim.zero_grad()\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            out = model(x)\n",
    "            \n",
    "            if not isinstance(out, torch.Tensor):\n",
    "                out = out[0] #  when features are also returned in forward\n",
    "            \n",
    "            loss = criterion(out, y)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            losses.append(loss.cpu().detach().numpy())\n",
    "            \n",
    "            batch_seen += 1\n",
    "            metric_result = test_loop(model, testdata, device)\n",
    "            tq.set_description(f'Epoch: {epoch_num}, Loss: {str(np.mean(losses))}, ROC-AUC: {metric_result}')\n",
    "            history.append(metric_result)\n",
    "                \n",
    "    return history\n",
    "\n",
    "def test_loop(model, testdata, device='cuda', return_loss=False):\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    metric = BinaryAUROC(device=device)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    loss = 0.0\n",
    "\n",
    "    for x, y in testdata:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        out = model(x)\n",
    "        if not isinstance(out, torch.Tensor):\n",
    "            out = out[0] #  when features are also returned in forward\n",
    "        pred = out.argmax(-1)\n",
    "        metric.update(pred, y)\n",
    "        if return_loss:\n",
    "            loss += criterion(out, y).detach().cpu().item()\n",
    "    \n",
    "    metric_result = metric.compute().item()\n",
    "    \n",
    "    model.train()\n",
    "    if return_loss:\n",
    "        return loss\n",
    "    \n",
    "    return metric_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42be33b6",
   "metadata": {},
   "source": [
    "Напишем функцию для создания полной модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17c9ce84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(device='cuda'):\n",
    "    trainloader, testloader = get_dataloaders([8,9], batch_size=64)\n",
    "    full_model = resnet50(pretrained=True)\n",
    "    full_model.fc = torch.nn.Linear(full_model.fc.in_features, 2)\n",
    "    train_loop(full_model, trainloader, testloader)\n",
    "    full_model = full_model.to(device)\n",
    "\n",
    "    return full_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583ca415",
   "metadata": {},
   "source": [
    "### Создание графа"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e8be9b",
   "metadata": {},
   "source": [
    "Инициализируем модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a24eebc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170M/170M [00:01<00:00, 91.3MB/s] \n",
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
      "100%|██████████| 97.8M/97.8M [00:00<00:00, 169MB/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61e9141401094563a7b70ccaf9eefb2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = get_model(device='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8adb70",
   "metadata": {},
   "source": [
    "**Из документации**\n",
    "\n",
    "Dependency Graph (DepGraph) is the core feature of Torch-Pruning, which provides an automatic mechanism to group dependent layers. There are two key concepts for DepGraph:\n",
    "\n",
    "- `tp.dependency.Dependency`: the dependency between layers.\n",
    "- `tp.dependency.DependencyGraph`: A relational graph to model the dependency.\n",
    "- `tp.dependency.Group`: A list of dependencies that represents the minimally-removable units."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2475f32a",
   "metadata": {},
   "source": [
    "Построим DependencyGraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9cb1fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "DG = tp.DependencyGraph().build_dependency(model, example_inputs=torch.randn(1,3,224,224))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0d55f6",
   "metadata": {},
   "source": [
    "У него есть ноды."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "287a2a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = list(DG.module2node.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a22a789c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Node: (_ElementWiseOp_3(ReluBackward0))>]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes[3].inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7db15ca4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Node: (_ElementWiseOp_6(AddBackward0))>,\n",
       " <Node: (layer4.1.conv1 (Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)))>]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes[10].outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d206754f",
   "metadata": {},
   "source": [
    "У его есть Dependency (Edge of DepGraph).\n",
    "\n",
    "For the dependency A -> B, the pruning operation ``trigger(A)`` will trigger \n",
    "the pruning operation ``handler(B)``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "61569024",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[prune_out_channels on _ElementWiseOp_1(TBackward0) => prune_in_channels on fc (Linear(in_features=2048, out_features=2, bias=True))]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes[2].dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ba793c",
   "metadata": {},
   "source": [
    "У этого объекта есть такие атрибуты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "28b93e66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Node: (_ElementWiseOp_1(TBackward0))>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes[2].dependencies[0].source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "46a2d7c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Node: (fc (Linear(in_features=2048, out_features=2, bias=True)))>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes[2].dependencies[0].target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e36024fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DummyPruner.prune_out_channels of <torch_pruning.ops.ElementWisePruner object at 0x7f24ad334b10>>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes[2].dependencies[0].trigger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04e6299",
   "metadata": {},
   "source": [
    "Dependency складываются в Group для прунинга.\n",
    "\n",
    "Group is the basic unit for pruning. It contains a list of dependencies and their corresponding indices.\n",
    "\n",
    "    group := [ (Dep1, Indices1), (Dep2, Indices2), ..., (DepK, IndicesK) ]\n",
    "\n",
    "Example: \n",
    "\n",
    "For a simple network Conv2d(2, 4) -> BN(4) -> Relu, we have:\n",
    "\n",
    "    group1 := [ (Conv2d -> BN, [0, 1, 2, 3]), (BN -> Relu, [0, 1, 2, 3]) ]\n",
    "\n",
    "There are 4 prunable elements, i.e., 4 channels in Conv2d.\n",
    "\n",
    "The indices do not need to be full and can be a subset of the prunable elements.\n",
    "For instance, if we want to prune the first 2 channels, we have:\n",
    "\n",
    "    group2 := [ (Conv2d -> BN, [0, 1]), (BN -> Relu, [0, 1]) ]\n",
    "\n",
    "When combined with tp.importance, we can compute the importance of corresponding channels.\n",
    "\n",
    "    imp_1 = importance(group1) # len(imp_1)=4\n",
    "    imp_2 = importance(group2) # len(imp_2)=2\n",
    "\n",
    "For importance estimation, we should craft a group with full indices just like group1.\n",
    "For pruning, we need to craft a new group with the to-be-pruned indices like group2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49ef3dd",
   "metadata": {},
   "source": [
    "Можно доставать Dependency из модуля, например."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ae53144b",
   "metadata": {},
   "outputs": [],
   "source": [
    "group = DG.get_pruning_group(model.layer1[0].conv1, pruning_fn=tp.prune_conv_out_channels, idxs=[2, 6, 9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "03abdb91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Dep:  prune_out_channels on layer1.0.conv1 (Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)) => prune_out_channels on layer1.0.conv1 (Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False))\n",
      " > Trigger:  <bound method ConvPruner.prune_out_channels of <torch_pruning.pruner.function.ConvPruner object at 0x7f24c5c1d550>>\n",
      " > Handler:  <bound method ConvPruner.prune_out_channels of <torch_pruning.pruner.function.ConvPruner object at 0x7f24c5c1d550>>\n",
      " > Source Layer:  Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      " > Target Layer:  Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "\n",
      "For Dep:  prune_out_channels on layer1.0.conv1 (Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)) => prune_out_channels on layer1.0.bn1 (BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
      " > Trigger:  <bound method ConvPruner.prune_out_channels of <torch_pruning.pruner.function.ConvPruner object at 0x7f24c5c1d550>>\n",
      " > Handler:  <bound method BatchnormPruner.prune_out_channels of <torch_pruning.pruner.function.BatchnormPruner object at 0x7f24c6076d90>>\n",
      " > Source Layer:  Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      " > Target Layer:  BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "\n",
      "For Dep:  prune_out_channels on layer1.0.bn1 (BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)) => prune_out_channels on _ElementWiseOp_38(ReluBackward0)\n",
      " > Trigger:  <bound method BatchnormPruner.prune_out_channels of <torch_pruning.pruner.function.BatchnormPruner object at 0x7f24c6076d90>>\n",
      " > Handler:  <bound method DummyPruner.prune_out_channels of <torch_pruning.ops.ElementWisePruner object at 0x7f24ad334b10>>\n",
      " > Source Layer:  BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      " > Target Layer:  _ElementWiseOp_38(ReluBackward0)\n",
      "\n",
      "For Dep:  prune_out_channels on _ElementWiseOp_38(ReluBackward0) => prune_in_channels on layer1.0.conv2 (Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False))\n",
      " > Trigger:  <bound method DummyPruner.prune_out_channels of <torch_pruning.ops.ElementWisePruner object at 0x7f24ad334b10>>\n",
      " > Handler:  <bound method ConvPruner.prune_in_channels of <torch_pruning.pruner.function.ConvPruner object at 0x7f24c5c1d550>>\n",
      " > Source Layer:  _ElementWiseOp_38(ReluBackward0)\n",
      " > Target Layer:  Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, (dep, idxs) in enumerate(group):\n",
    "    trigger = dep.trigger\n",
    "    handler = dep.handler\n",
    "    source_layer = dep.source.module\n",
    "    target_layer = dep.target.module\n",
    "\n",
    "    print(\"For Dep: \", dep)\n",
    "    print(\" > Trigger: \", trigger)\n",
    "    print(\" > Handler: \", handler)\n",
    "    print(\" > Source Layer: \", source_layer)\n",
    "    print(\" > Target Layer: \", target_layer)\n",
    "    print(\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
